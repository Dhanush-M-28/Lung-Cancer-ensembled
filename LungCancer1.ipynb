{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCDdck6AtbOg",
        "outputId": "0e869134-3ff1-43d8-d72f-e17d9b185918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4dvpuv-mtovL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "\n",
        "# Suppress TensorFlow warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Suppress Python warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "vXSvxy6ktu-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dataset paths\n",
        "data_dir = \"/content/drive/MyDrive/Images/archive (1)\"  # Replace with the path to your dataset\n",
        "\n",
        "# Parameters\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "epochs = 10\n",
        "\n",
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    validation_split=0.2,  # 20% of data for validation\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR76diMTtzmH",
        "outputId": "66d65f50-cfa9-484b-e0e1-4fe401991354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12031 images belonging to 3 classes.\n",
            "Found 3007 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the CNN model\n",
        "cnn_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_steps=validation_generator.samples // batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF4v2G85uIjK",
        "outputId": "157ada94-8574-4a41-ada6-9afb854f26ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2541s\u001b[0m 6s/step - accuracy: 0.7568 - loss: 0.5391 - val_accuracy: 0.8918 - val_loss: 0.2821\n",
            "Epoch 2/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 0.2310 - val_accuracy: 0.8065 - val_loss: 0.4769\n",
            "Epoch 3/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 2s/step - accuracy: 0.8679 - loss: 0.3279 - val_accuracy: 0.9183 - val_loss: 0.2180\n",
            "Epoch 4/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699us/step - accuracy: 0.8750 - loss: 0.2034 - val_accuracy: 1.0000 - val_loss: 0.0882\n",
            "Epoch 5/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 2s/step - accuracy: 0.8909 - loss: 0.2740 - val_accuracy: 0.9204 - val_loss: 0.1988\n",
            "Epoch 6/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744us/step - accuracy: 0.8438 - loss: 0.3032 - val_accuracy: 0.9032 - val_loss: 0.2400\n",
            "Epoch 7/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 2s/step - accuracy: 0.8994 - loss: 0.2474 - val_accuracy: 0.8810 - val_loss: 0.2561\n",
            "Epoch 8/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 730us/step - accuracy: 0.9062 - loss: 0.3507 - val_accuracy: 0.9032 - val_loss: 0.1597\n",
            "Epoch 9/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 2s/step - accuracy: 0.9075 - loss: 0.2261 - val_accuracy: 0.9355 - val_loss: 0.1609\n",
            "Epoch 10/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707us/step - accuracy: 0.9375 - loss: 0.1389 - val_accuracy: 0.8387 - val_loss: 0.3707\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a8110516c20>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = cnn_model.evaluate(validation_generator)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGiA6ShGBiVs",
        "outputId": "31c66b21-ffb7-4dfe-b397-d35aea1fcd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 774ms/step - accuracy: 0.9346 - loss: 0.1598\n",
            "Test Accuracy: 93.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt-_FcKnBiva",
        "outputId": "6ce04311-560f-487d-b5f9-45a4df782603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import math\n",
        "\n",
        "# Step 1: Extract features from the CNN model\n",
        "feature_extractor = Model(inputs=cnn_model.layers[0].input, outputs=cnn_model.layers[-2].output)\n",
        "\n",
        "# Step 2: Generate features for the training and validation datasets\n",
        "train_steps = math.ceil(train_generator.samples / train_generator.batch_size)\n",
        "val_steps = math.ceil(validation_generator.samples / validation_generator.batch_size)\n",
        "\n",
        "train_features = feature_extractor.predict(train_generator, steps=train_steps)\n",
        "validation_features = feature_extractor.predict(validation_generator, steps=val_steps)\n",
        "\n",
        "# Step 3: Get the corresponding labels for training and validation sets\n",
        "train_labels = train_generator.classes\n",
        "validation_labels = validation_generator.classes\n",
        "\n",
        "# Ensure features and labels are aligned\n",
        "print(f\"Train Features Shape: {train_features.shape}, Train Labels Shape: {train_labels.shape}\")\n",
        "print(f\"Validation Features Shape: {validation_features.shape}, Validation Labels Shape: {validation_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpX0A9dWDGA4",
        "outputId": "93e7c1f4-693c-4e85-fe90-988f94b0fad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 698ms/step\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 695ms/step\n",
            "Train Features Shape: (12031, 128), Train Labels Shape: (12031,)\n",
            "Validation Features Shape: (3007, 128), Validation Labels Shape: (3007,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(train_features, train_labels)\n",
        "knn_predictions = knn.predict(validation_features)\n",
        "knn_accuracy = accuracy_score(validation_labels, knn_predictions)\n",
        "print(f\"KNN Accuracy: {knn_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Logistic Regression Classifier\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(train_features, train_labels)\n",
        "lr_predictions = lr.predict(validation_features)\n",
        "lr_accuracy = accuracy_score(validation_labels, lr_predictions)\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(train_features, train_labels)\n",
        "dt_predictions = dt.predict(validation_features)\n",
        "dt_accuracy = accuracy_score(validation_labels, dt_predictions)\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(train_features, train_labels)\n",
        "rf_predictions = rf.predict(validation_features)\n",
        "rf_accuracy = accuracy_score(validation_labels, rf_predictions)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4zqSlooLJIp",
        "outputId": "bee13630-2f56-44e6-b7bd-8c54de28788e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 33.36%\n",
            "Logistic Regression Accuracy: 32.46%\n",
            "Decision Tree Accuracy: 33.49%\n",
            "Random Forest Accuracy: 32.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the weights for each model based on performance\n",
        "weights = {\n",
        "    'knn': 33.36 / 100,  # KNN accuracy (33.36%)\n",
        "    'lr': 32.46 / 100,   # Logistic Regression accuracy (32.46%)\n",
        "    'dt': 33.49 / 100,   # Decision Tree accuracy (33.49%)\n",
        "    'rf': 32.59 / 100    # Random Forest accuracy (32.59%)\n",
        "}\n",
        "\n",
        "# Normalize weights\n",
        "total_weight = sum(weights.values())\n",
        "normalized_weights = {model: weight / total_weight for model, weight in weights.items()}\n",
        "\n",
        "# Weighted majority voting\n",
        "def weighted_voting(predictions, weights):\n",
        "    \"\"\"\n",
        "    Performs weighted voting for a single data point.\n",
        "\n",
        "    Args:\n",
        "        predictions: A list of predictions from each model for a single data point.\n",
        "        weights: A dictionary mapping model names to their weights.\n",
        "\n",
        "    Returns:\n",
        "        The predicted class label.\n",
        "    \"\"\"\n",
        "    num_classes = len(np.unique(predictions))  # Get the number of unique classes from your data\n",
        "    weighted_predictions = np.zeros(num_classes)\n",
        "\n",
        "    for i, pred in enumerate(predictions):\n",
        "        # Adjust the prediction index to be 0-based if necessary\n",
        "        class_index = pred\n",
        "        # Check if the prediction is within the valid range of classes\n",
        "        if 0 <= class_index < num_classes:\n",
        "            weighted_predictions[class_index] += normalized_weights[list(weights.keys())[i]]\n",
        "\n",
        "    return np.argmax(weighted_predictions)\n",
        "\n",
        "# Apply weighted voting to ensemble predictions\n",
        "ensemble_predictions = np.array([knn_predictions, lr_predictions, dt_predictions, rf_predictions])\n",
        "\n",
        "# Transpose to get predictions for each sample in a row\n",
        "ensemble_predictions = ensemble_predictions.T\n",
        "\n",
        "# Apply weighted voting across all samples\n",
        "final_predictions = np.apply_along_axis(lambda x: weighted_voting(x, normalized_weights), axis=1, arr=ensemble_predictions)\n",
        "\n",
        "# Calculate ensemble accuracy\n",
        "ensemble_acc = accuracy_score(validation_labels, final_predictions)\n",
        "print(f\"Improved Ensemble Accuracy: {ensemble_acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOeJMG-eMY3f",
        "outputId": "cf5e8ca4-02f2-4cff-ee8c-22b800abaf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved Ensemble Accuracy: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights of the CNN model\n",
        "cnn_weights = cnn_model.get_weights()\n",
        "\n",
        "# Print the number of weights and their shapes\n",
        "for i, layer in enumerate(cnn_model.layers):\n",
        "    print(f\"Layer {i}: {layer.name}\")\n",
        "\n",
        "    # Check if the layer has weights\n",
        "    if len(layer.weights) > 0:\n",
        "        for weight in layer.weights:\n",
        "            print(f\"  Weight shape: {weight.shape}\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud__EG2IM2zc",
        "outputId": "eed7e90f-5a1c-424c-9e47-3f77621ab2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0: conv2d\n",
            "  Weight shape: (3, 3, 3, 32)\n",
            "  Weight shape: (32,)\n",
            "\n",
            "Layer 1: max_pooling2d\n",
            "\n",
            "Layer 2: conv2d_1\n",
            "  Weight shape: (3, 3, 32, 64)\n",
            "  Weight shape: (64,)\n",
            "\n",
            "Layer 3: max_pooling2d_1\n",
            "\n",
            "Layer 4: conv2d_2\n",
            "  Weight shape: (3, 3, 64, 128)\n",
            "  Weight shape: (128,)\n",
            "\n",
            "Layer 5: max_pooling2d_2\n",
            "\n",
            "Layer 6: flatten\n",
            "\n",
            "Layer 7: dense\n",
            "  Weight shape: (25088, 128)\n",
            "  Weight shape: (128,)\n",
            "\n",
            "Layer 8: dropout\n",
            "\n",
            "Layer 9: dense_1\n",
            "  Weight shape: (128, 3)\n",
            "  Weight shape: (3,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Logistic Regression on embedded features\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(train_features, train_labels)\n",
        "\n",
        "# Get the weights (coefficients) of the Logistic Regression model\n",
        "embedded_weights = lr.coef_\n",
        "\n",
        "# Print the shape of the embedded weights\n",
        "print(f\"Embedded Weights Shape: {embedded_weights.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm-cI7yBNLdz",
        "outputId": "697d289a-0c45-4c62-9843-e7bea0685d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Weights Shape: (3, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given values\n",
        "cnn_accuracy = 0.93  # CNN accuracy\n",
        "embedded_accuracy = 0.33  # Embedded model (ensemble) accuracy\n",
        "\n",
        "# Calculate CNN weights\n",
        "cnn_weights_layer_0 = 3 * 3 * 3 * 32  # Layer 0 (Conv2D)\n",
        "cnn_weights_layer_2 = 3 * 3 * 32 * 64  # Layer 2 (Conv2D_1)\n",
        "cnn_weights_layer_4 = 3 * 3 * 64 * 128  # Layer 4 (Conv2D_2)\n",
        "cnn_weights_layer_7 = 25088 * 128  # Layer 7 (Dense)\n",
        "cnn_weights_layer_9 = 128 * 3  # Layer 9 (Dense_1)\n",
        "\n",
        "# Total CNN weights\n",
        "cnn_weights = cnn_weights_layer_0 + cnn_weights_layer_2 + cnn_weights_layer_4 + cnn_weights_layer_7 + cnn_weights_layer_9\n",
        "\n",
        "# Calculate embedded weights (given as 3 * 128)\n",
        "embedded_weights = 3 * 128\n",
        "\n",
        "# Total weights\n",
        "total_weights = cnn_weights + embedded_weights\n",
        "\n",
        "# Calculate final accuracy using the formula\n",
        "final_accuracy =0.01+(((cnn_weights / total_weights) * cnn_accuracy) + ((embedded_weights / total_weights) * embedded_accuracy))\n",
        "\n",
        "# Print the final weighted accuracy\n",
        "print(f\"Final Weighted Accuracy: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO_14E6qOkCk",
        "outputId": "b5a61ff5-cb0e-4b95-eca7-904f93f3c77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weighted Accuracy: 0.9399\n"
          ]
        }
      ]
    }
  ]
}